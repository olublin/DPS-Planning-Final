{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4a5583-d187-4680-a1c1-441ab885e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e8577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broad_filter(res_cases):\n",
    "    # filter out cases with irrelevant type codes\n",
    "    types = ['PL_MINSP', 'PL_SSP_SM', 'PL_SSM_SM2', 'PL_CPAA', 'PL_MINPP', 'PL_MAJSP', 'PL_MAJSUP', 'PL_PPA', 'PL_MAJPP']\n",
    "    filter_cases_type = res_cases[res_cases['A_TYPE'].isin(types)]\n",
    "\n",
    "    # filter out cases with out of date status\n",
    "    status = res_cases['A_STATUS'].unique()\n",
    "    status = status[~np.isin(status, ['WITH', 'VOID','DEN','DISAP','EXP'])]\n",
    "    filter_cases_status = filter_cases_type[filter_cases_type['A_STATUS'].isin(status)]\n",
    "\n",
    "    # keep entries with keywords\n",
    "    keywords = ['home', 'family', 'residen', 'mixed', 'mized', 'duplex', 'apartment', ' housing', 'condo', 'dwelling', 'tenant', 'affordable', 'units', 'townhouse']\n",
    "    pattern = '|'.join(keywords)\n",
    "    filtered_in = filter_cases_status[filter_cases_status['A_DESCRIPT'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "    # remove entries with certain words\n",
    "    keywords_avoid = ['expand','storage']\n",
    "    pattern_avoid = '|'.join(keywords_avoid)\n",
    "    filtered_words = filtered_in[~filtered_in['A_DESCRIPT'].str.contains(pattern_avoid, case=False, na=False)]\n",
    "\n",
    "    # filter out entries that were last updated over 5 years ago\n",
    "    filtered_words = filtered_words.copy()\n",
    "    filtered_words['A_STATUS_D'] = pd.to_datetime(filtered_words['A_STATUS_D'])\n",
    "    filtered_final = filtered_words[filtered_words['A_STATUS_D'].dt.year>=2020]\n",
    "\n",
    "    return filtered_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45613bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_for_regex(term):\n",
    "    # makes it so string returns a match whether a term has spaces, dashes, both, or neither\n",
    "    return re.sub(r'[-\\s]+', r'\\\\s*-?\\\\s*', term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98718ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_units(description):\n",
    "    # remove square footage references\n",
    "    description = re.sub(\n",
    "        r'(\\d+|\\d{1,3}(,\\d{3})*)(\\s+[A-Za-z-]+){0,2}?\\s*(SF|square feet|sq\\.?\\s*ft\\.?|sqft)',\n",
    "        '', description, flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # map variations to standardized types\n",
    "    term_map = {\n",
    "        \"home\": \"home\", \"homes\": \"home\", \"house\": \"home\", \"houses\": \"home\",\n",
    "        \"duplex\": \"duplex\", \"duplexes\": \"duplex\",\n",
    "        \"condo\": \"condo\", \"condominium\": \"condo\", \"condominiums\": \"condo\", \"condos\": \"condo\", \n",
    "        \"apartment\": \"apartment\", \"apartments\": \"apartment\",\n",
    "        \"townhome\": \"townhouse\", \"townhomes\": \"townhouse\",\n",
    "        \"townhouse\": \"townhouse\", \"townhouses\": \"townhouse\",\n",
    "        \"town home\": \"townhouse\", \"town homes\": \"townhouse\",\n",
    "        \"town house\": \"townhouse\", \"town houses\": \"townhouse\",\n",
    "        \"multifamily\": \"multifamily\", \"multi-family\": \"multifamily\", \n",
    "        \"multi - family\": \"multifamily\", \"multi family\": \"multifamily\",\n",
    "        \"mutifamily\": \"multifamily\", \"MF\": \"multifamily\",\n",
    "        \"single family\": \"single family\", \"single-family\": \"single family\", \n",
    "        \"single - family\": \"single family\", \"s-f\": \"single family\", \"s - f\": \"single family\", \"s f\": \"single family\"\n",
    "    }\n",
    "\n",
    "    modifiers = [\"attached\", \"detached\"]\n",
    "    suffixes = [\"units\", \"lots\", \"homes\", \"houses\"]\n",
    "\n",
    "    housing_pattern = \"|\".join([normalize_for_regex(term) for term in term_map])\n",
    "    modifier_pattern = \"|\".join(modifiers)\n",
    "    suffix_pattern = \"|\".join(suffixes)\n",
    "\n",
    "    # extended match pattern to support both \"qty before type\" and \"type before qty\"\n",
    "    match_pattern = rf'''\n",
    "    (?:\n",
    "        # Qty before type\n",
    "        (?P<qty>\\(?\\d{{1,4}}\\)?)\n",
    "        (?:\\s*[-+&/]?\\s*)?\n",
    "        (?:({modifier_pattern})\\s*){{0,2}}?\n",
    "        (?:\\w+\\s*){{0,4}}?\n",
    "        (?P<type>{housing_pattern})\n",
    "        (?:\\s+({modifier_pattern}))?\n",
    "        (?:\\s+(?P<suffix>{suffix_pattern}))?\n",
    "\n",
    "    |\n",
    "        # Type before qty\n",
    "        (?P<type2>{housing_pattern})\n",
    "        (?:\\s+({modifier_pattern}))?\n",
    "        (?:\\s*[-+&/]?\\s*)?\n",
    "        (?:\\w+\\s*){{0,4}}?\n",
    "        (?P<qty2>\\(?\\d{{1,4}}\\)?)\n",
    "        (?:\\s+(?P<suffix2>{suffix_pattern}))?\n",
    "\n",
    "    |\n",
    "        # Type with quantity in parentheses\n",
    "        (?P<type3>{housing_pattern})\n",
    "        (?:\\s+\\w+){{0,4}}?\n",
    "        \\(\\s*(?P<qty3>\\d{{1,4}})\\s+(?P<suffix3>{suffix_pattern})\\s*\\)\n",
    "    )\n",
    "'''\n",
    "\n",
    "    matches = re.finditer(match_pattern, description, flags=re.IGNORECASE | re.VERBOSE)\n",
    "    \n",
    "    result = []\n",
    "    for match in matches:\n",
    "        qty = match.group(\"qty\") or match.group(\"qty2\")\n",
    "        raw_type = match.group(\"type\") or match.group(\"type2\")\n",
    "        raw_mod = match.group(2)  # first modifier (position varies)\n",
    "        raw_suffix = match.group(\"suffix\") or match.group(\"suffix2\")\n",
    "\n",
    "        if not qty or not raw_type:\n",
    "            continue  # skip malformed matches\n",
    "\n",
    "        # normalize type\n",
    "        norm_key = re.sub(r'[-\\s]+', ' ', raw_type.lower()).strip()\n",
    "        normalized_type = term_map.get(norm_key, norm_key)\n",
    "\n",
    "        result.append((\n",
    "            int(qty.strip(\"()\")),\n",
    "            raw_mod.lower() if raw_mod else None,\n",
    "            normalized_type,\n",
    "            raw_suffix.lower() if raw_suffix else None\n",
    "        ))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7fa7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_types(match_results):\n",
    "    housing_types = ['sf_detached', 'sf_attached', 'duplex/triplex', 'multifamily', 'condo']\n",
    "    housing_type_dict = {\n",
    "        'townhouse': 'sf_attached',\n",
    "        'home': 'sf_detached', 'single family': 'sf_detached',\n",
    "        'duplex': 'duplex/triplex',\n",
    "        'apartment': 'multifamily', 'multifamily': 'multifamily',\n",
    "        'condo': 'condo'\n",
    "    }\n",
    "\n",
    "    row_data = {h_type: 0 for h_type in housing_types}\n",
    "    for group in match_results:\n",
    "        quantity = group[0]\n",
    "        mod = group[1]\n",
    "        housing = group[2]\n",
    "\n",
    "        if housing == 'single family' and mod == 'attached':\n",
    "            row_data['sf_attached'] += quantity\n",
    "        elif housing in housing_type_dict:\n",
    "            row_data[housing_type_dict[housing]] += quantity\n",
    "\n",
    "    return pd.Series(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718c5fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input the name of the Durham developments shapefile:  durham_developments\n"
     ]
    }
   ],
   "source": [
    "durham_dev_filename = input('Please input the name of the Durham developments shapefile: ').strip()\n",
    "res_cases_raw = gpd.read_file(f'../data/{durham_dev_filename}')\n",
    "res_filtered = broad_filter(res_cases_raw)\n",
    "res_filtered['match_results'] = res_filtered['A_DESCRIPT'].apply(extract_units)\n",
    "housing_counts = res_filtered['match_results'].apply(fill_types)\n",
    "filtered_final = pd.concat([res_filtered, housing_counts], axis=1)\n",
    "filtered_final = filtered_final.to_crs(epsg = 3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25e43375-e7cb-4bea-aeea-56b4fadf304c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the file name which includes the table of SGRs by housing type and region:  sgr_tables_htype_reg.csv\n"
     ]
    }
   ],
   "source": [
    "# read data from Data+_2025/data/enrollment_projections/sgr_table_region_2324_20240710.xlsx in Google Drive\n",
    "'''\n",
    "read in SGR data -- file paths: \n",
    "the current one is from 2024 July 10th, the file is already in data and is named sgr_tables_htype_reg.xlsx\n",
    "'''\n",
    "sgr_filename = input('Please enter the file name which includes the table of SGRs by housing type and region: ')\n",
    "sgr_data = gpd.read_file(f'../data/{sgr_filename}')\n",
    "sgr_data = sgr_data[sgr_data['region']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4526c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null values and shorten to only use relevant columns\n",
    "sgr_data.rename(columns={'sgr_dps_2324_all.1': 'sgr_dps_avg_k12'}, inplace=True) # because there might be a typo in the file?\n",
    "sgr_data = sgr_data[['housing_type','region','sgr_dps_avg_k12']]\n",
    "sgr_data['sgr_dps_avg_k12'] = sgr_data['sgr_dps_avg_k12'].round(4)\n",
    "sgr_data.set_index(['region', 'housing_type'], inplace=True)\n",
    "sgr_data['sgr_dps_avg_k12'] = pd.to_numeric(sgr_data['sgr_dps_avg_k12'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e7d353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A_NUMBER', 'A_TYPE', 'A_DATE', 'A_STATUS', 'A_STATUS_D', 'A_PROJECT_',\n",
       "       'A_DESCRIPT', 'A_USER_ID', 'A_CASE_PLA', 'StatCode', 'AppStatus',\n",
       "       'AppCode', 'AppType', 'CasePlanne', 'EMAIL', 'ORIG_FID', 'CreationDa',\n",
       "       'Creator', 'EditDate', 'Editor', 'geometry', 'match_results',\n",
       "       'sf_detached', 'sf_attached', 'duplex/triplex', 'multifamily', 'condo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c13de98f-1e3a-4250-8e81-fc112879f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read in shapefile to get geometries for Durham County regions from Data+_2025/QGIS/DPS shapefiles from layers in Google Drive\n",
    "'''\n",
    "regions = gpd.read_file(r'../data/durham_regions.geojson')[['region', 'geometry']]\n",
    "regions = regions.to_crs(epsg = 3857)\n",
    "#read in geojson with residential developments\n",
    "# ''' \n",
    "# read in geojson with residential developments -- file paths: \n",
    "# Leah: /Users/leahwallihan/Durham_school_planning/DPS-Planning/GIS_files/resdev_cases.geojson'''\n",
    "# res_dev = gpd.read_file(r'/Users/kevan/OneDrive/Desktop/Data+/DPS-Planning/GIS_files/resdev_cases.geojson')\n",
    "# res_dev = res_dev.to_crs('EPSG:4326')\n",
    "# res_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06b3a132-de08-4dcf-b061-644d2319c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of students in each row\n",
    "def count_students(row): \n",
    "        \n",
    "    htype_map = {\n",
    "        'sf_detached': 'sf_detach',\n",
    "        'sf_attached': 'sf_attach',\n",
    "        'duplex/triplex': 'du_tri',\n",
    "        'multifamily': 'mf_apt',\n",
    "        'condo': 'condo'\n",
    "    }\n",
    "\n",
    "    region = row['region']\n",
    "\n",
    "    total = 0\n",
    "    for col_name, sgr_col in htype_map.items():\n",
    "        count = row.get(col_name, 0)\n",
    "\n",
    "        try:\n",
    "            multiplier = sgr_data.loc[(region, sgr_col), 'sgr_dps_avg_k12']\n",
    "        except KeyError:\n",
    "            multiplier = 0\n",
    "\n",
    "        total += count * multiplier\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c56ef292",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'region'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialdata/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'region'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filtered_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent_gen\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filtered_final\u001b[38;5;241m.\u001b[39mapply(count_students, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialdata/lib/python3.12/site-packages/geopandas/geodataframe.py:1834\u001b[0m, in \u001b[0;36mGeoDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;129m@doc\u001b[39m(pd\u001b[38;5;241m.\u001b[39mDataFrame)\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, result_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, args\u001b[38;5;241m=\u001b[39m(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1834\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m   1835\u001b[0m         func, axis\u001b[38;5;241m=\u001b[39maxis, raw\u001b[38;5;241m=\u001b[39mraw, result_type\u001b[38;5;241m=\u001b[39mresult_type, args\u001b[38;5;241m=\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1836\u001b[0m     )\n\u001b[1;32m   1837\u001b[0m     \u001b[38;5;66;03m# Reconstruct gdf if it was lost by apply\u001b[39;00m\n\u001b[1;32m   1838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1839\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(result, DataFrame)\n\u001b[1;32m   1840\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_geometry_column_name \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   1841\u001b[0m     ):\n\u001b[1;32m   1842\u001b[0m         \u001b[38;5;66;03m# axis=1 apply will split GeometryDType to object, try and cast back\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialdata/lib/python3.12/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialdata/lib/python3.12/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialdata/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialdata/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mcount_students\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount_students\u001b[39m(row): \n\u001b[1;32m      4\u001b[0m     htype_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_detached\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_detach\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_attached\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_attach\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondo\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondo\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m     }\n\u001b[0;32m---> 12\u001b[0m     region \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col_name, sgr_col \u001b[38;5;129;01min\u001b[39;00m htype_map\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialdata/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialdata/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialdata/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'region'"
     ]
    }
   ],
   "source": [
    "filtered_final['student_gen'] = filtered_final.apply(count_students, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a504f427-8ffd-4394-bb94-c5e7dff523e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_final[['region', 'sf_detached', 'sf_attached', 'multifamily', 'student_gen','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3a8a90c3-8066-4283-81b9-7ea0cd3fd490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input the geodataframe with all planning units:  pu_2324_SPLIT.geojson\n"
     ]
    }
   ],
   "source": [
    "#read in the planning units\n",
    "pu_filename = input('Please input the geodataframe with all planning units: ')\n",
    "dps_pu = gpd.read_file(f'../data/{pu_filename}').rename(columns={'pu_2324_848':'pu_2324_84'})\n",
    "dps_pu = dps_pu.to_crs(epsg = 3857).sort_values(by='pu_2324_84')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "29e52550-7980-419c-b9e6-cf1e8f3894c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_final = filtered_final.copy()\n",
    "for i,geometry in enumerate(dps_pu['geometry']):\n",
    "    in_geometry = geometry.contains(filtered_final['geometry'])\n",
    "    pu = dps_pu.loc[i,'pu_2324_84']\n",
    "    filtered_final.loc[in_geometry,'pu_2324_84'] = pu\n",
    "filtered_final = filtered_final.groupby('pu_2324_84')['student_gen'].sum().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cbb6affe-355d-480e-a7fd-5c156a38b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed835da9-543d-4176-8840-2b99ff685326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in the current enrollment for only the 2024-25 school year\n",
    "enrollment_filename = input('Please enter the file name for the file with current enrollment by planning unit by year and grade: ')\n",
    "current_enrollment = gpd.read_file(f'../data/{enrollment_filename}').rename(columns={'pu_2324_848':'pu_2324_84'})\n",
    "current_enrollment = current_enrollment[['pu_2324_84','grade','fall_year','basez']].replace('', 0)\n",
    "current_enrollment[['pu_2324_84','grade','fall_year','basez']] = current_enrollment[['pu_2324_84','grade','fall_year','basez']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50443e4d-1494-496a-8f33-d1d9982cc759",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_type = input('Would you like to get the enrollment projections for elementary, middle, or high schools? Enter as es, ms, or hs: ')\n",
    "\n",
    "if school_type == 'es':\n",
    "    grades = [0,1,2,3,4,5]\n",
    "elif school_type == 'ms':\n",
    "    grades = [6,7,8]\n",
    "elif school_type == 'hs':\n",
    "    grades = [9,10,11,12]\n",
    "\n",
    "current_by_type = current_enrollment[\n",
    "    (current_enrollment['grade'].isin(grades)) &\n",
    "    (current_enrollment['fall_year'].isin([2022,2023,2024]))\n",
    "     ]\n",
    "\n",
    "averaged_by_type = current_by_type.groupby(['pu_2324_84','grade'],as_index=False).mean()\n",
    "averaged_by_type = averaged_by_type.groupby(['pu_2324_84'],as_index=False).sum().drop(columns=['grade','fall_year'])\n",
    "\n",
    "all_pus = pd.DataFrame({'pu_2324_84': range(1,852)})\n",
    "full_basez = all_pus.merge(averaged_by_type, on='pu_2324_84',how='left').fillna(0)\n",
    "\n",
    "full_basez.loc[773,'basez'] = full_basez.loc[773,'basez'] * 30.0/81.0\n",
    "full_basez.loc[850,'basez'] = full_basez.loc[773,'basez'] * 51.0/81.0\n",
    "full_basez[['basez']] = full_basez[['basez']].map(lambda x: int(x))\n",
    "full_basez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "06c75a79-55bb-463a-984b-68ddf5bd3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_geo = dps_pu.merge(full_basez, on = 'pu_2324_84')[['pu_2324_84'\"\",'Region','geometry','basez']]\n",
    "full_geo = full_geo.merge(filtered_final,on='pu_2324_84')\n",
    "full_geo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88724739-e6a0-41f1-ad2b-053b98922a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spatialdata]",
   "language": "python",
   "name": "conda-env-spatialdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
